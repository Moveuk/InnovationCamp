# 20220714 TIL

## 📝 웹개발 플러스 과정 - 3주차 내용 정리

### 웹 크롤링 vs 웹 스크리핑

 한국에서는 이 두 용어를 혼용하여 쓰는 법이 많다고 한다. 실제로 그래서 최근 velog 글에서도 사람들이 생각하는 웹 크롤링 정의에 대한 불만?을 표출하는 글도 보았다. 사실은 불만이라기보다 웹 크롤링이라고 표현하나 결국은 단순 스크래핑 수준에 걸치는 고도화 되지 않은 저수준의 코드 덩이에 대한 이야기인 것 같았다. 해당 velog을 아래에 첨부한다.

-> [`웹 크롤러` 좀 그만 만들어라](https://velog.io/@mowinckel/%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81-I)

 위 글에서도 볼 수 있듯 크롤링이란 작업은 어떤 특정 정보를 찾기 위해서 `자동화`하여 `주기적`으로 웹 상의 페이지들을 돌아다니며 `색인/분류`하고 업데이트된 부분을 찾는 등의 다양하고 고도화된 자동화 기능이라는 것을 말한다.

 이와 달리 웹 스크래핑은 단순히 어떤 페이지에서 우리가 원하는 부분의 데이터를 수집하는 것을 말한다.


<br>

### project03 환경 세팅

|Package            |Version|
|------------------|-----------|
|beautifulsoup4     |4.11.1     |
|bs4                |0.0.1      |
|certifi            |2022.6.15  |
|charset-normalizer |2.1.0      |
|click              |8.1.3      |
|colorama           |0.4.5      |
|dnspython          |2.2.1      |
|Flask              |2.1.2      |
|idna               |3.3|
|importlib-metadata |4.12.0|
|itsdangerous       |2.1.2|
|Jinja2             |3.1.2|
|MarkupSafe         |2.1.1|
|pip                |20.2.1|
|pymongo            |4.1.1|
|requests           |2.28.1|
|setuptools         |49.2.1|
|soupsieve          |2.3.2.post1|
|urllib3            |1.26.9|
|Werkzeug           |2.1.2|
|zipp               |3.8.0|

<br>

### selenium을 활용한 스크래핑

 -> 멜론과 같은 동적인 사이트를 스크래핑할 때는 값이 정확하게 나오지 않게 된다. 예를들어 멜론의 like 좋아요 값은 동적이므로 단순히 다음 코드로 불러올 때 값이 제대로 안들어오게된다.

![image](https://user-images.githubusercontent.com/84966961/179012019-ab1a964b-80b6-4298-a197-ce2feefce097.png)

 ```python
 url = "https://www.melon.com/chart/day/index.htm"
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}
data = requests.get(url, headers=headers)

req = data.text
soup = BeautifulSoup(req, 'html.parser')

songs = soup.select("#frm > div > table > tbody > tr")
print(len(songs))

for song in songs:
    title = song.select_one("td > div > div.wrap_song_info > div.rank01 > span > a").text
    artist = song.select_one("td > div > div.wrap_song_info > div.rank02 > span > a").text
    likes = song.select_one("td > div > button.like > span.cnt").text
    print(title, artist, likes)
```

 고로 jinja2 와 같이 서버사이드 렌더링 방식이 아닌 html 내부의 값들을 긁어올 수 없다. 그렇기에 페이지 렌더링을 위해서 웹브라우저를 통제할 수 있는 selenium을 사용하여 크롬 드라이버를 깔고 데이터를 받아올 수 있도록 한다.


<br>

 ### selenium 사용 세팅

 1. 크롬 드라이버를 깔기 위해 현재 크롬 브라우저 버전을 확인한다.(`chrome://settings/help`에서 확인가능)
    - 본인 버전은 `버전 103.0.5060.114(공식 빌드) (64비트)`이다.
 2. `https://chromedriver.storage.googleapis.com/index.html` 드라이버 사이트에서 본인 버전과 가장 가까운 드라이버를 각 운영체제에 맞게 다운받아 설치한다.
 3. 받은 드라이버를 사용할 `scraping.py` 파일과 같은 경로에 넣어준다.
 4. 스크래핑 파일을 실행한다.

<br>

 ### selenium 사용

드라이버가 각 url을 접속하여 기다렸다 html을 받아 종료한다.(동적 데이터를 모두 받은 html을 받아오기 위해서) 실제로 드라이버가 작동되면 크롬이 켜졌다 꺼지게 된다.

```python
driver.get(url)  # 드라이버에 해당 url의 웹페이지를 띄웁니다.
sleep(5)  # 페이지가 로딩되는 동안 5초 간 기다립니다.

req = driver.page_source  # html 정보를 가져옵니다.
driver.quit()  # 정보를 가져왔으므로 드라이버는 꺼줍니다.
```

실제 데이터 출력됨.

```text
100
그때 그 순간 그대로 (그그그) WSG워너비 (가야G) 
총건수
54,849
보고싶었어 WSG워너비 (4FIRE) 
총건수
50,349
LOVE DIVE IVE (아이브) 
총건수
162,855
```

html 구조를 분리하여 string만 받아오는 방법이다. 먼저 `decompose()`로 태그를 날리고, `strip()`으로 공백을 없애서 다음 코드 결과와 같이 보이게 한다.
```python
for song in songs:
    title = song.select_one("td > div > div.wrap_song_info > div.rank01 > span > a").text
    artist = song.select_one("td > div > div.wrap_song_info > div.rank02 > span > a").text
    likes_tag = song.select_one("td > div > button.like > span.cnt")
    likes_tag.span.decompose()  # span 태그 없애기
    likes = likes_tag.text.strip()  # 텍스트화한 후 앞뒤로 빈 칸 지우기
    print(title, artist, likes)
```
```text
100
그때 그 순간 그대로 (그그그) WSG워너비 (가야G) 54,856
보고싶었어 WSG워너비 (4FIRE) 50,355
LOVE DIVE IVE (아이브) 162,855
```

<hr>

같은 방법으로 naver image를 검색해서 각 이미지의 html 주소값을 받아올 수도 있다. 특히 이런 상황에서 힘을 발휘하는데 구글이나 네이버에서 이미지를 검색할 경우 무한 스크롤 방식을 이용해서 스크롤을 내림에 따라서 반응형으로 이미지들이 append 되는 걸 확인할 수 있다. selenium과 크롬 드라이버를 활용하여 창을 키고 3초 기다린 후 1000픽셀 내려서 새로운 이미지를 부르고 1초후에 다시 해당 창의 끝 스크롤까지 내려 이미지를 더 받는 방법을 다음 코드와 같이 할 수 있다.

```python
driver = webdriver.Chrome('./chromedriver')

url = "https://search.naver.com/search.naver?where=image&sm=tab_jum&query=%EC%95%84%EC%9D%B4%EC%9C%A0"
driver.get(url)
sleep(3)
driver.execute_script("window.scrollTo(0, 1000)")  # 1000픽셀만큼 내리기 -> 이미지가 새로생김
sleep(1)
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);") # 현재 스크롤 끝까지 내리기 -> 새로운 이미지들이 또 검색됨
sleep(10)

req = driver.page_source
driver.quit()

soup = BeautifulSoup(req, 'html.parser')
images = soup.select(".tile_item._item ._image._listImage")
print(len(images))

for image in images:
    src = image["src"]
    print(src)
```

<br>

